# Gestion SEO - Sitemap et Robots.txt

Ce projet inclut une gestion dynamique du SEO avec sitemap.xml et robots.txt g√©n√©r√©s automatiquement.

## üöÄ Fonctionnalit√©s

- **Sitemap automatique** : G√©n√®re automatiquement un sitemap bas√© sur vos pages Plasmic
- **Robots.txt dynamique** : D√©sindexe les pages que vous ne voulez pas voir dans les moteurs de recherche
- **Configuration centralis√©e** : G√©rez tout depuis un seul fichier de configuration

## üìÅ Fichiers cr√©√©s

- `pages/api/robots.txt.ts` - API route pour robots.txt
- `pages/api/sitemap.xml.ts` - API route pour sitemap.xml
- `utils/seo-config.ts` - Configuration centralis√©e
- `next.config.mjs` - Mis √† jour avec les redirections

## üîß Configuration

### 1. Variables d'environnement

Ajoutez √† votre fichier `.env.local` :

```bash
NEXT_PUBLIC_SITE_URL=https://votre-domaine.com
```

### 2. Pages √† d√©sindexer

Modifiez le fichier `utils/seo-config.ts` pour ajouter/retirer des pages :

```typescript
DISALLOWED_PAGES: [
  '/test-file-list',    // Page de test
  '/plasmic-host',      // Page host Plasmic
  '/admin/*',           // Toutes les pages admin
  '/private/*',         // Toutes les pages priv√©es
  '/ma-page-privee',    // Page sp√©cifique
]
```

## üìç URLs d'acc√®s

Une fois d√©ploy√©, vos fichiers seront accessibles √† :

- `https://votre-domaine.com/robots.txt`
- `https://votre-domaine.com/sitemap.xml`

## üîç Comment √ßa marche

### Robots.txt
- D√©sindexe les pages list√©es dans `DISALLOWED_PAGES`
- Supporte les patterns avec `*` (ex: `/admin/*`)
- Inclut automatiquement le lien vers le sitemap

### Sitemap.xml
- R√©cup√®re automatiquement toutes les pages Plasmic via `PLASMIC.fetchPages()`
- Exclut les pages list√©es dans `DISALLOWED_PAGES`
- Met √† jour automatiquement la date de derni√®re modification
- Cache les r√©sultats pendant 24h pour les performances

## üõ†Ô∏è Personnalisation

### Changer la fr√©quence de mise √† jour

```typescript
SITEMAP_CONFIG: {
  changefreq: 'daily', // weekly, monthly, yearly
  defaultPriority: 0.8,
  homePriority: 1.0,
}
```

### Ajouter des pages personnalis√©es

Si vous avez des pages en dehors de Plasmic, modifiez `pages/api/sitemap.xml.ts` :

```typescript
// Ajoutez vos pages manuelles
const manualPages = [
  { path: '/contact', priority: 0.9 },
  { path: '/about', priority: 0.8 },
]
```

## üöÄ D√©ploiement

1. **Ajoutez la variable d'environnement** sur votre plateforme (Vercel, Netlify, etc.)
2. **Red√©ployez** votre application
3. **Testez** en visitant `/robots.txt` et `/sitemap.xml`

## üìä Validation

### Outils de test
- [Google Search Console](https://search.google.com/search-console) - Pour tester le sitemap
- [Robots.txt Tester](https://www.google.com/webmasters/tools/robots-testing-tool)
- [XML Sitemap Validator](https://www.xml-sitemaps.com/validate-xml-sitemap.html)

### Commandes de v√©rification
```bash
# Tester en local
curl http://localhost:3001/robots.txt
curl http://localhost:3001/sitemap.xml
```

## üîÑ Maintenance

Les fichiers se mettent √† jour automatiquement :
- **Cache** : 24h pour les performances
- **Donn√©es** : Bas√©es sur les pages Plasmic actuelles
- **Configuration** : Modifiez `utils/seo-config.ts` selon vos besoins

## ‚ö†Ô∏è Notes importantes

- Les pages sont r√©cup√©r√©es dynamiquement depuis Plasmic
- Le cache de 24h am√©liore les performances
- Les patterns avec `*` permettent d'exclure des dossiers entiers
- Pensez √† soumettre votre sitemap √† Google Search Console apr√®s d√©ploiement 